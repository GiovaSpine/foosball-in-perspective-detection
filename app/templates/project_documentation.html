{% extends "base.html" %}

<!-- ========================================================================== -->

{% block head %}
    <link rel="stylesheet" href="{{ url_for('static', filename='info_pages_style.css') }}">
{% endblock %}

<!-- ========================================================================== -->

{% block content %}
<h2>Project Documentation</h2>

<section>
    <p>
        <strong>Project Documentation</strong> provides a comprehensive overview of the work
        carried out in this project, covering the following aspects:
        <ul>
            <li>Project Overview</li>
            <li>Dataset</li>
            <li>Data Analysis</li>
            <li>Data Augmentation</li>
            <li>Training Details</li>
        </ul>
    </p>
</section>

<!-- Project Documentation -->
<section>

    <section">
        <h3>Project Overview</h3>
        <p>
            This project uses a YOLO-based keypoint detection model to identify the eight vertices of a foosball table, given an image
            of a match.
        </p>
        <p>
            Many other projects that try to find the position of the ball, to be able to control a robot to play for example,
            require images taken from above the table, and so mapping a point from image coordinates to foosball table coordinates
            is an easy task.
        </p>
        <p>  
            But sometimes is not feasible to place a camera in such a place, and most videos of foosball matches are taken from
            a <strong>spectator view</strong>.
        </p>
        <p>
            For <strong>spectator view</strong> we refer to the view you would have by looking at the match from the short side of the table,
            while other people are playing, occupying the long sides.
        </p>
        <p>
            Examples:
        </p>
        <div class="gallery">
            <img src="/static/images/page-images/Specator Image 1.png">
            <img src="/static/images/page-images/Specator Image 2.png">
            <img src="/static/images/page-images/Specator Image 3.png">
            <img src="/static/images/page-images/Specator Image 4.png">
        </div>
        <p>
            So the goal is to use an AI model to detect the vertices automatically and use some algorithm to translate the positions.
        </p>
        <p>
            Additionally, instead of trying to find only the four vertices of the play area (the lower ones), that are the only ones needed
            for translation, we can try to find all the eight vertices of the volume, like in the image below,
            so that we can have a deeper understanding and more capabilities, like estimating the position of the player's lines
            (the cylinders where the small football player statues are located).
            <img src="/static/images/page-images/Index.png">
        </p>

        
    </section>

    <section>
        <h3>Dataset</h3>
        <p>
            The dataset is composed of 1,128 images of foosball matches captured from different spectator viewpoints.
            The images exhibit a wide variability in camera angles, table models, acquisition environments, camera devices
            and zoom levels, as well as lighting conditions.
        </p>
        
        <p>
            The annotations were made using a self made annotation tool, designed to precisely select keypoint positions, 
            following perspective constraints.
            This aspect is particularly important, as in most spectator viewpoints the model is required to infer the positions
            of vertices 6 and 7, which are often occluded by obstacles, as shown in the example below.
        </p>

        <p>
            For the keypoints order we have the following rules:
            <ul>
                <li>
                    Since the images are captured from a spectator's view, the two highest vertices of the table
                    (corresponding to the lowest row in the image) are vertices of the upper rectangle, and 
                    are assigned as the first two keypoints: keypoint 0 on the left and keypoint 1 on the right.
                </li>
                <li>
                    The other two keypoints (2 and 3) are the last two vertices of the upper rectangle, and follow a
                    clockwise order.
                </li>
                <li>
                    For the lower rectangle, that corresponds to the play area, we follow the same rules, but start
                    from 4 and end with 7 instead.
                </li>
            </ul>
            Example:
            <img src="/static/images/page-images/Keypoint Order.png">
        </p>

        <p>
            In every image of the dataset, the first four keypoints are always present and never fall outside
            the image boundaries. This represents a design constraint of the project: when these four keypoints
            are present, at least one or two of the remaining lower keypoints are also present,
            allowing the others to be calculated by applying perspective constraints.
        </p>
        
    </section>


    <section>
        <h3>Data Analysis</h3>
        <p>
            The dataset was originally composed of 681 images. Using K-Means, the images were grouped into different clusters,
            and additional images were added based on their visual properties in order to increase the dataset size while
            attempting to balance clusters with fewer samples.
        </p>
        <p>
            Some of the observations are summarized below:
            <ul>
                <li>
                    With K=2, the clusters appeared to show that the number of images captured with a zoomed camera (high focal length)
                    were approximately 200 fewer than the images taken with a regular zoom (human-level focal length).
                    <div class="small_gallery">
                        <img src="/static/images/samples/index104_frame_015.jpg">
                        <img src="/static/images/samples/index83_frame_012.jpg">
                    </div>
                </li>
                <li>
                    With K=3, the cluster corresponding to zoomed-camera images was further divided
                    into light and dark images, with the lighter images being slightly more numerous.
                    <div class="small_gallery">
                        <img src="/static/images/samples/Screenshot (476).png">
                        <img src="/static/images/samples/index102_frame_006.jpg">
                    </div>
                </li>
                <li>
                    With K=4 a very small cluster of 39 images, with strangely rotated images appeared. and stayed the same in every other K.
                    This issue was addressed through rotation-based data augmentation techniques.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index57_frame_013.jpg">
                    </div>
                </li>
                <li>
                    With K=5, a small cluster of approximately 70 very dark images appeared, suggesting the need for additional samples.
                    Another cluster of around 100 images, captured from a top-down viewpoint above the table, was also identified and
                    suggested a need of slightly more top-down viewpoint above the table.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index88_frame_017.jpg">
                        <img src="/static/images/samples/Alto 9.png">
                    </div>
                </li>
                <li>
                    With K=6, the largest cluster, composed of regularly zoomed images taken from a human-level height, was split into
                    two sub-clusters: one containing white tables with green play areas, and another with darker-colored tables.
                    The first one contained approximately 50 fewer images than the second one.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index2_frame_016.jpg">
                        <img src="/static/images/samples/index19_frame_004.jpg">
                    </div>
                </li>
            </ul>
        </p>

        <hr>
        
        <p>
            Based on the K-Means and annotations analysis, an additional 447 images were
            added to the dataset. The clustering procedure was then repeated to evaluate whether
            the resulting clusters became more balanced.
        </p>

        <p>
            For each value of K in {2, 3, 4, 5, 6}, the graphs below compare the distribution of images across
            clusters before and after the dataset expansion. The plots on the left show the
            cluster counts for the original dataset of 681 images, while the plots on the right
            illustrate the effect of the added images.
            <div class="gallery">
                <img src="/static/images/graphs/default_cluster_counts_image_k_2.png">
                <img src="/static/images/graphs/added_cluster_counts_image_k_2.png">
            </div>
            <div class="gallery">
                <img src="/static/images/graphs/default_cluster_counts_image_k_3.png">
                <img src="/static/images/graphs/added_cluster_counts_image_k_3.png">
            </div>
            <div class="gallery">
                <img src="/static/images/graphs/default_cluster_counts_image_k_4.png">
                <img src="/static/images/graphs/added_cluster_counts_image_k_4.png">
            </div>
            <div class="gallery">
                <img src="/static/images/graphs/default_cluster_counts_image_k_5.png">
                <img src="/static/images/graphs/added_cluster_counts_image_k_5.png">
            </div>
            <div class="gallery">
                <img src="/static/images/graphs/default_cluster_counts_image_k_6.png">
                <img src="/static/images/graphs/added_cluster_counts_image_k_6.png">
            </div>
        </p>

        <hr>

        <p>
            The full dataset was also analyzed in terms of shape, resolution, aspect ratio, brightness, and saturation.
            The resulting distributions were used to guide the data augmentation process in order to achieve a more balanced dataset.
        </p>

        <p>
            The graphs and some of the observations are summarized below:
            <ul>
                <li>
                    Shapes and Aspect Ratio
                    <p>
                        <strong>Note:</strong> No images were padded to obtain shapes different from the original photograph.
                    </p>
                    <p>
                        The analysis shows that the vast majority of images are <em>horizontal rectangle</em>,
                        with an aspect ratio around 1.77, and so will have to apply crop-based transformations to
                        mitigate this big discrepancy.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Shape (Added).png">
                        <img src="/static/images/graphs/Ratio (Added).png">
                    </div>
                </li>
                <li>
                    Resolution
                    <p>
                        We define <em>high-resolution</em> images as those whose longest side is greater than 640 pixels. Such images are
                        downscaled by YOLO during preprocessing. Conversely, <em>low-resolution</em> images are upscaled using
                        interpolation, which may introduce artifacts and potential bias.
                    </p>
                    <p>
                        The analysis shows that the vast majority of images are <em>high-resolution</em>,
                        and so we will have to apply resizing transformations to mitigate this big discrepancy.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Resolution (Added).png">
                    </div>
                    
                </li>
                <li>
                    Brightness and Saturation
                    <p>
                        The analysis of the averagebrightness and saturation distributions indicates a relatively well-balanced dataset.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Brightness (Added).png">
                        <img src="/static/images/graphs/Saturation (Added).png">
                    </div>
                </li>
            </ul>
        </p>

        <hr>

        <p>
            The annotations allowed an analysis of the table's positions, directions, sizes and keypoint visibilities .
            The resulting distributions were used to guide the data augmentation process in order to achieve a more balanced dataset.
        </p>

        <p>
            The graphs and some of the observations are summarized below:
            <ul>
                <li>
                    Centers
                    <p>
                        For <em>center</em> we refer to the intersection between the line that goes from the first keypoint to
                        the third, and the line that goes from the second to the fourth.
                        The coordinates are normalized, and we can see obviously that most foosball tables are around the center
                        of the image.
                    </p>
                    <p>
                        With data augmentation we will try to apply scale and traslation transformations to generate augmented images
                        in a not centered position.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Centers (Added).png">
                        <img src="/static/images/graphs/Centers Heatmap (Added).png">
                    </div>
                </li>
                <li>
                    Directions
                    <p>
                        For <em>direction</em> we refer to the vector that goes from the center of the table to the vanishing point
                        for the y axis, calculated as the intersection between the line that goes from the first keypoint to the fourth,
                        and the line that goes from the second to the third.
                    </p>
                    <p>
                        In the following image below, the direction is rapresented as the green y arrow.
                    </p>
                    <div class="small_gallery">
                        <img src="/static/images/page-images/Translated Point.png">
                    </div>
                    <p>
                        From the direction we can calculate the angle, and we can see that the vast majority of images have an angle around 90
                        degrees, that corresponds to a vertical direction.
                    </p>
                    <p>
                        With data augmentation we will have to mitigate this big discrepancy, applying rotation-based transformations.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Directions (Added).png">
                        <img src="/static/images/graphs/Theta (Added).png">
                    </div>
                </li>
                <li>
                    Size
                    <p>
                        For <em>size</em> or <em>dimension</em> we refer to the percentage of how much the bounding box covers the image.
                    </p>
                    <p>
                        The graph indicates that most images have a bounding box that covers around 25% of the image, and so with
                        data augmentation we will apply scale-based transformations to mitigate this discrepancy.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/BBox Dimension (Added).png">
                    </div>
                </li>
                <li>
                    Visibilities
                    <p>
                        <strong>Reminder:</strong>
                        <ul>
                            <li>A visibility of 2 indicates that the keypoint is visible in the image.</li>
                            <li>A visibility of 1 indicates that the keypoint is present in the image, but occluded by something.</li>
                            <li>A visibility of 0 indicates that the keypoint outside the boundaries of the image.</li>
                        </ul>
                    </p>
                    <p>
                        The analysis shows that the vast majority of the first four keypoints have a visibility of 2, and only a small
                        fraction are occluded by an obstacle, while for the last 4 keypoints it's the opposite.
                    </p>
                    <p>
                        During data augmentation we will give a higher priority to be chosen to generate new data, to images
                        with at least one of the first four keypoints with a visibility of 1, or with at least one of the last four
                        keypoints with a visibility of 0 or 2.
                    </p>
                    <div class="gallery">
                        <img src="/static/images/graphs/Visibilities (Added).png">
                    </div>
                </li>
            </ul>
        </p>

    </section>

    <section>
        <h3>Data Augmentation</h3>

        <p>
            Data augmentation was guided not only by the distributions obtained from the image and annotation analysis,
            but also by the informations provided by K-Means clustering.
        </p>
        <p>
            A first data augmentation was selectively applied to images belonging to clusters with significantly fewer samples
            compared to the others, with the goal of achieving a more balanced cluster distribution.
            The applied transformations were not drastic, in order to ensure that the augmented images
            would still belong to the same cluster as their original source images.
        </p>
        <p>
            For each value of K in {2, 3, 4, 5, 6}, the graphs below compare the distribution of images across
            clusters before and after the data augmentation guided by K-Means. The plots on the left show the
            cluster counts for the full dataset of 1,128 images, while the plots on the right
            illustrate the effect of the data augmentation.
        </p>
        <div class="gallery">
            <img src="/static/images/graphs/added_cluster_counts_image_k_2.png">
            <img src="/static/images/graphs/clustering_augmented_cluster_counts_image_k_2.png">
        </div>
        <div class="gallery">
            <img src="/static/images/graphs/added_cluster_counts_image_k_3.png">
            <img src="/static/images/graphs/clustering_augmented_cluster_counts_image_k_3.png">
        </div>
        <div class="gallery">
            <img src="/static/images/graphs/added_cluster_counts_image_k_4.png">
            <img src="/static/images/graphs/clustering_augmented_cluster_counts_image_k_4.png">
        </div>
        <div class="gallery">
            <img src="/static/images/graphs/added_cluster_counts_image_k_5.png">
            <img src="/static/images/graphs/clustering_augmented_cluster_counts_image_k_5.png">
        </div>
        <div class="gallery">
            <img src="/static/images/graphs/added_cluster_counts_image_k_6.png">
            <img src="/static/images/graphs/clustering_augmented_cluster_counts_image_k_6.png">
        </div>
        <p>
            The results indicate that for K in {2, 3, 6} the applied data augmentation did not improve
            the balance between clusters, whereas for K in {4, 5} a more balanced distribution was achieved.
        </p>

        <br>

        <p>
            K-Means clustering was also applied with K values ranging from 7 to 20.
            The graphs below compare the distribution of images across clusters before (on the left) and after (on the right)
            data augmentation, similarly to the previous analysis, but allow all cluster counts to be visualized simultaneously.
        </p>
        <p>
            Each row represents a specific number of clusters K, ranging from 2 to 20.
            Each square corresponds to a cluster for that value of K and is color-coded to indicate
            whether its size is above or below the average cluster size for that row (yellow colors indicate
            balanced distributions).
        </p>
        <div class="big_gallery">
            <img src="/static/images/graphs/added_all_counts.png">
            <img src="/static/images/graphs/clustering_augmented_all_counts.png">
        </div>
        <p>
            Consistently with the earlier observations, for smaller values of K such as 2, 3, and 6,
            the data augmentation does not appear to significantly improve cluster balance.
            However, for higher values of K, the augmented data graph on the right shows less extreme colors,
            indicating a clearer balancing effect.
        </p>

        <hr>

        <p>
            The data augmentation guided by distributions obtained from the image and annotation analysis
            was applied <strong>after</strong> the one guided by K-Means, just discussed.
        </p>

        <p>
            On the left, the previous data analysis graphs are shown, while on the right the effect of this final
            data augmentation step is presented. This comparison highlights how the overall data distribution changes and
            provides insight into how these modifications may influence the model's training behavior and generalization performance,
            which will be discussed in the following section.
        </p>
        
        <ul>
            <li>
                Shape and Aspect Ratio
                <p>
                    The graphs indicate that, although data augmentation improves the balance of shapes and aspect ratios,
                    a significant discrepancy still remains. As a result, the model is expected to perform better on
                    horizontally oriented rectangular images, particularly those with an aspect ratio close to 1.77,
                    which remain the most represented in the dataset.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Shape (Added).png">
                    <img src="/static/images/graphs/Shape (Augmented).png">
                </div>
                <div class="gallery">
                    <img src="/static/images/graphs/Ratio (Added).png">
                    <img src="/static/images/graphs/Ratio (Augmented).png">
                </div>
            </li>
            <li>
                Resolution
                <p>
                    The graphs show an improved balance between high and low resolution images after data augmentation
                    and therefore, the model is expected to exhibit more consistent performance across different resolution levels.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Resolution (Added).png">
                    <img src="/static/images/graphs/Resolution (Augmented).png">
                </div>
            </li>
            <li>
                Brightness and Saturation
                <p>
                    The augmented data graphs appear slightly more balanced than the original ones, which were already well balanced
                    prior to data augmentation. This indicates that the augmentation process further reinforced an already stable distribution
                    rather than correcting a major imbalance.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Brightness (Added).png">
                    <img src="/static/images/graphs/Brightness (Augmented).png">
                </div>
                <div class="gallery">
                    <img src="/static/images/graphs/Saturation (Added).png">
                    <img src="/static/images/graphs/Saturation (Augmented).png">
                </div>
            </li>
            <li>
                Centers
                <p>
                    The data augmentation process increased the coverage of table positions within the image.
                    However, since the majority of the dataset still contains foosball tables located near the center of the image,
                    it is reasonable to expect the model to achieve its best performance when the table is positioned
                    close to the image center.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Centers (Added).png">
                    <img src="/static/images/graphs/Centers (Augmented).png">
                </div>
                <div class="gallery">
                    <img src="/static/images/graphs/Centers Heatmap (Added).png">
                    <img src="/static/images/graphs/Centers Heatmap (Augmented).png">
                </div>
            </li>
            <li>
                Directions
                <p>
                    The graph shows that data augmentation increased the range of table directions covered in the dataset.
                    However, the number of images at these extreme angles remains limited, and the overall angle distribution
                    appears largely unchanged. As a result, the model is expected to perform poorly for table that
                    deviate significantly from a vertical direction around 90 degrees.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Directions (Added).png">
                    <img src="/static/images/graphs/Directions (Augmented).png">
                </div>
                <div class="gallery">
                    <img src="/static/images/graphs/Theta (Added).png">
                    <img src="/static/images/graphs/Theta (Augmented).png">
                </div>
            </li>
                
            <li>
                Size
                <p>
                    The graph shows a more evenly distributed range of foosball table sizes,
                    indicating an improved balance after data augmentation.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/BBox Dimension (Added).png">
                    <img src="/static/images/graphs/BBox Dimension (Augmented).png">
                </div>
            </li>
            <li>
                Visibilities
                <p>
                    The data augmentation graph appears largely unchanged, indicating that this particular imbalance
                    was not addressed. As a consequence, the model is expected to perform poorly in scenarios where
                    one of the first four keypoints is occluded.
                    Additionally, degraded performance is also expected when at least one of keypoints 6 or 7 has zero visibility,
                    meaning it lies outside the image boundaries.
                </p>
                <div class="gallery">
                    <img src="/static/images/graphs/Visibilities (Added).png">
                    <img src="/static/images/graphs/Visibilities (Augmented).png">
                </div>
            </li>
        </ul>
        
        <p>
            At the end, the data augmentation produced a dataset of 8,303 images, from an original
            dataset of 1,128 images.
        </p>

    </section>

    <section>
        <h3>Training Details</h3>
        <p>
            The model used for this project is <strong>YOLOv8 nano</strong>, configured for keypoint detection.
            This architecture was chosen instead of <strong>YOLOv11 nano</strong>, because it might have
            been too complex for the limited dataset size, potentially causing overfitting.
        </p>
        
        <p>
            The dataset was divided into a training set of 6,190 images and a validation set of 2,113 images, with no test set,
            because a further division of the data could have reduced the training set size too much, and therefore no
            hyperparameter fine-tuning was performed.
        </p>

        <p>
            The division was made using the clusters obtained with K-Means clustering with K=20.
            Each cluster was assigned to either the training or validation set in order to reach the desired set sizes.
            The idea is that different clusters capture different visual characteristics and therefore, this strategy helps
            ensure that the validation set does not contain images that are too similar to those seen during training.
        </p>

        <p>
            Check out the model's performance in the <a href="{{ url_for('results') }}">Results</a> section.
        </p>

    </section>
</section>

{% endblock %}