{% extends "base.html" %}

<!-- ========================================================================== -->

{% block head %}
    <link rel="stylesheet" href="{{ url_for('static', filename='info_pages_style.css') }}">
{% endblock %}

<!-- ========================================================================== -->

{% block content %}
<h2>Documentation</h2>

<section>
    <p>
        The documentation is divided into two main sections:
    </p>
    <ul>
        <li><strong>Project Documentation:</strong> covers the project overview, dataset, data analysis,  and training details.</li>
        <li><strong>API Documentation:</strong> describes all backend endpoints, including how to use them, input formats and output responses.</li>
    </ul>
</section>

<!-- Project Documentation -->
<section>
    <h2>Project Documentation</h2>

    <section">
        <h3>Project Overview</h3>
        <p>
            This project uses a YOLO-based keypoint detection model to identify the eight vertices of a foosball table, given an image
            of a match.
        </p>
        <p>
            Many other projects that try to find the position of the ball, to be able to control a robot to play for example,
            require images taken from above the table, and so mapping a point from image coordinates to foosball table coordinates
            is an easy task.
        </p>
        <p>  
            But sometimes is not feasible to place a camera in such a place, and most videos of foosball matches are taken from
            a <strong>spectator view</strong>.
        </p>
        <p>
            For <strong>spectator view</strong> we refer to the view you would have by looking at the match from the short side of the table,
            while other people are playing, occupying the long sides.
        </p>
        <p>
            Examples:
        </p>
        <div class="gallery">
            <img src="/static/images/page-images/Specator Image 1.png">
            <img src="/static/images/page-images/Specator Image 2.png">
            <img src="/static/images/page-images/Specator Image 3.png">
            <img src="/static/images/page-images/Specator Image 4.png">
        </div>
        <p>
            So the goal is to use an AI model to detect the vertices automatically and use some algorithm to translate the positions.
        </p>
        <p>
            Additionally, instead of trying to find only the four vertices of the play area (the lower ones), that are the only ones needed
            for translation, we can try to find all the eight vertices of the volume, like in the image below,
            so that we can have a deeper understanding and more capabilities, like estimating the position of the player's lines
            (the cylinders where the small football player statues are located).
            <img src="/static/images/page-images/Index.png">
        </p>

        
    </section>

    <section>
        <h3>Dataset</h3>
        <p>
            The dataset is composed of 1,128 images of foosball matches captured from different spectator viewpoints.
            The images exhibit a wide variability in camera angles, table models, acquisition environments, camera devices
            and zoom levels, as well as lighting conditions.
        </p>
        
        <p>
            The annotations were made using a self made annotation tool, designed to precisely select keypoint positions, 
            following perspective constraints.
            This aspect is particularly important, as in most spectator viewpoints the model is required to infer the positions
            of vertices 6 and 7, which are often occluded by obstacles, as shown in the example below.
        </p>

        <p>
            For the keypoints order we have the following rules:
            <ul>
                <li>
                    Since the images are captured from a spectator's view, the two highest vertices of the table
                    (corresponding to the lowest row in the image) are vertices of the upper rectangle, and 
                    are assigned as the first two keypoints: keypoint 0 on the left and keypoint 1 on the right.
                </li>
                <li>
                    The other two keypoints (2 and 3) are the last two vertices of the upper rectangle, and follow a
                    clockwise order.
                </li>
                <li>
                    For the lower rectangle, that corresponds to the play area, we follow the same rules, but start
                    from 4 and end with 7 instead.
                </li>
            </ul>
            Example:
            <img src="/static/images/page-images/Keypoint Order.png">
        </p>

        <p>
            In every image of the dataset, the first four keypoints are always present and never fall outside
            the image boundaries. This represents a design constraint of the project: when these four keypoints
            are present, at least one or two of the remaining lower keypoints are also present,
            allowing the others to be calculated by applying perspective constraints.
        </p>
        
    </section>


    <section>
        <h3>Data Analysis</h3>
        <p>
            The dataset was originally composed of 681 images. Using K-Means, the images were grouped into different clusters,
            and additional images were added based on their visual properties in order to increase the dataset size while
            attempting to balance clusters with fewer samples.
        </p>
        <P>
            Some of the observations are summarized below:
            <ul>
                <li>
                    With K=2, the clusters appeared to show that the number of images captured with a zoomed camera (high focal length)
                    were approximately 200 fewer than the images taken with a regular zoom (human-level focal length).
                    <div class="small_gallery">
                        <img src="/static/images/samples/index104_frame_015.jpg">
                        <img src="/static/images/samples/index83_frame_012.jpg">
                    </div>
                </li>
                <li>
                    With K=3, the cluster corresponding to zoomed-camera images was further divided
                    into light and dark images, with the lighter images being slightly more numerous.
                    <div class="small_gallery">
                        <img src="/static/images/samples/Screenshot (476).png">
                        <img src="/static/images/samples/index102_frame_006.jpg">
                    </div>
                </li>
                <li>
                    With K=4 a very small cluster of 39 images, with strangely rotated images appeared. and stayed the same in every other K.
                    This issue was addressed through rotation-based data augmentation techniques.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index57_frame_013.jpg">
                    </div>
                </li>
                <li>
                    With K=5, a small cluster of approximately 70 very dark images appeared, suggesting the need for additional samples.
                    Another cluster of around 100 images, captured from a top-down viewpoint above the table, was also identified and
                    suggested a need of slightly more top-down viewpoint above the table.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index88_frame_017.jpg">
                        <img src="/static/images/samples/Alto 9.png">
                    </div>
                </li>
                <li>
                    With K=6, the largest cluster, composed of regularly zoomed images taken from a human-level height, was split into
                    two sub-clusters: one containing white tables with green play areas, and another with darker-colored tables.
                    The first one contained approximately 50 fewer images than the second one.
                    <div class="small_gallery">
                        <img src="/static/images/samples/index2_frame_016.jpg">
                        <img src="/static/images/samples/index19_frame_004.jpg">
                    </div>
                </li>
            </ul>
        </P>
        
        <p>
            The annotations allowed an analysis of the table's positions, directions, sizes and keypoint visibilities .
            The resulting distributions were used not only to guide the data augmentation process, in order to achieve a more balanced dataset,
            but also to choose additional images to add to the dataset.
        </p>

        <P>
            The graphs and some of the observations are summarized below:
            <ul>
                <li>
                    Centers
                    <div class="gallery">
                        <img src="/static/images/graphs/Centers (Default).png">
                        <img src="/static/images/graphs/Centers Heatmap (Default).png">
                    </div>
                </li>
                <li>
                    Directions
                    <div class="gallery">
                        <img src="/static/images/graphs/Directions (Default).png">
                        <img src="/static/images/graphs/Theta (Default).png">
                    </div>
                </li>
                <li>
                    Size
                    <div class="gallery">
                        <img src="/static/images/graphs/BBox Dimension (Default).png">
                    </div>
                </li>
                <li>
                    Visibilities
                    <div class="gallery">
                        <img src="/static/images/graphs/Visibilities (Default).png">
                    </div>
                </li>
            </ul>
        </P>

        <p>With the analysis of K-Means and annotations additional 447 images were added to the dataset.</p>

        <p>
            The resulting dataset was also analyzed in terms of shape, resolution, aspect ratio, brightness, and saturation.
            The resulting distributions were used to guide the data augmentation process in order to achieve a more balanced dataset.
        </p>

    </section>

    <section>
        <h3>Training Details</h3>
        <p>
            The model used for this project is <strong>YOLOv8 nano</strong>, configured for keypoint detection.
            This architecture was chosen instead of <strong>YOLOv11 nano</strong>, because it might have
            been too complex for the limited dataset size, potentially causing overfitting.
        </p>
        
        <p>
            The dataset was divided into a training set of 6,190 images and a validation set of 2,113 images, with no test set,
            because a further division of the data could have reduced the training set size too much, and therefore no
            hyperparameter fine-tuning was performed.
        </p>

        <p>
            Check out the model's performance in the <a href="{{ url_for('results') }}">Results</a> section.
        </p>

    </section>
</section>

<!-- API Documentation -->
<section>
    <h2>API Documentation</h2>

    <section>
        <h3>/predict</h3>
        <p>
            Accepts image uploads via POST and returns the detected keypoints, bounding boxes, and confidence scores.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>Form field: <code>photo</code> (image file)</li>
            <li>Response JSON:
                <pre>
{
    "keypoints": [[x1, y1], [x2, y2], ...],
    "bounding_boxes": [[x, y, w, h], ...],
    "confidence": [...]
}</pre>
            </li>
        </ul>
    </section>

    <section>
        <h3>/clean-keypoints</h3>
        <p>
            Cleans raw predicted keypoints data by following perspective rules, to improve accuracy and consistency.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>JSON body: <pre>{"keypoints": [[x1, y1], ...]}</pre></li>
            <li>Response JSON: <pre>{"keypoints": [[x1, y1], ...]}</pre></li>
        </ul>
    </section>

    <section>
        <h3>/translate-position</h3>
        <p>
            Translates a point from image coordinates (pixel space) to normalized table coordinates.
            The transformation is computed using the last four detected keypoints, corresponding to
            the lower vertices of the table (play area).
        </p>

        <p>
            The input point must be expressed in pixel coordinates relative to the image.
            The output is a normalized 2D point in a coordinate system, where the origin is 
            located at the center of the play area and the <code>x</code> axis goes towards
            the line formed by the keypoint 5 and 6, and the <code>y</code> axis goes towards
            the line formed by the keypoint 4 and 5.
        </p>

        <p>Example:</p>
        <img src="/static/images/page-images/Translated Point.png">

        <ul>
            <li>Method: POST</li>
            <li>JSON body:
                <pre>
{
    "lower_keypoints": [[x1, y1], [x2, y2], ...],
    "point": [x, y]
}</pre>
            </li>
            <li>Response JSON: <pre>{"translated_point": [x, y]}</pre></li>
        </ul>
    </section>

    <section>
        <h3>/get-player-lines</h3>
        <p>
            Calculates the estimated positions of the foosball player lines based on all detected keypoints.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>JSON body: <pre>{"keypoints": [[x1, y1], [x2, y2], ...]}</pre></li>
            <li>Response JSON:
                <pre>
{
    "player_lines": [[[x1, y1], [x2, y2]], ...]
}</pre>
            </li>
        </ul>
    </section>
</section>
{% endblock %}