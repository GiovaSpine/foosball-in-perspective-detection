{% extends "base.html" %}

<!-- ========================================================================== -->

{% block head %}
    <link rel="stylesheet" href="{{ url_for('static', filename='info_pages_style.css') }}">
{% endblock %}

<!-- ========================================================================== -->

{% block content %}
<h2>Documentation</h2>

<section id="documentation-intro">
    <p>
        The documentation is divided into two main sections:
    </p>
    <ul>
        <li><strong>Project Documentation:</strong> covers the project overview, dataset and training details.</li>
        <li><strong>API Documentation:</strong> describes all backend endpoints, including how to use them, input formats and output responses.</li>
    </ul>
</section>

<!-- Project Documentation -->
<section>
    <h2>Project Documentation</h2>

    <section">
        <h3>Project Overview</h3>
        <p>
            This project uses a YOLO-based keypoint detection model to identify the eight vertices of a foosball table, given an image
            of a match.
        </p>
        <p>
            Many other projects that try to find the position of the ball, to be able to control a robot to play for example,
            require images taken from above the table, and so mapping a point from image coordinates to foosball table coordinates
            is an easy task.
        </p>
        <p>  
            But sometimes is not feasible to place a camera in such a place, and most videos of foosball matches are taken from
            a <strong>spectator view</strong>.
        </p>
        <p>
            For <strong>spectator view</strong> we refer to the view you would have by looking at the match from the short side of the table,
            while other people are playing, occupying the long sides.
        </p>
        <p>
            Examples:
        </p>
        <div class="gallery">
            <img src="/static/images/page-images/Specator Image 1.png">
            <img src="/static/images/page-images/Specator Image 2.png">
            <img src="/static/images/page-images/Specator Image 3.png">
            <img src="/static/images/page-images/Specator Image 4.png">
        </div>
        <p>
            So the goal is to use an AI model to detect the vertices automatically and use some algorithm to translate the positions.
        </p>
        <p>
            Additionally, instead of trying to find only the four vertices of the play area (the lower ones), that are the only ones needed
            for translation, we can try to find all the eight vertices of the volume, like in the image below,
            so that we can have a deeper understanding and more capabilities, like estimating the position of the player's lines
            (the cylinders where the small football player statues are located).
        </p>

        <img src="/static/images/page-images/Index.png">

    </section>

    <section>
        <h3>Dataset</h3>
        <p>
            The model was trained on 1130 images of foosball matches captured from a spectator's view.
        </p>
        
        <p>
            The annotations were made using a self made annotation tool, designed to precisely select keypoint positions, 
            following perspective constraints.
        </p>

        <p>
            For the keypoints order we have the following rules:
            <ul>
                <li>
                    Since the images are captured from a spectator's view, the two highest vertices of the table
                    (corresponding to the lowest row in the image) are vertices of the upper rectangle, and 
                    are assigned as the first two keypoints: keypoint 0 on the left and keypoint 1 on the right.
                </li>
                <li>
                    The other two keypoints (2 and 3) are the last two vertices of the upper rectangle, and follow a
                    clockwise order.
                </li>
                <li>
                    For the lower rectangle, that corresponds to the play area, we follow the same rules, but start
                    from 4 and end with 7 instead.
                </li>
            </ul>
            Example:
        </p>

        <img src="/static/images/page-images/Keypoint Order.png">

        <p>
            The images and were analyzed in terms of resolutions, shapes, ratios, brightnesses and saturations,
            and were divided into clusters using K-Means.
        </p>

        <p>
            The annotations were analyzed with respect to table orientation, position in the image and keypoint visibilities.
        </p>

        <p>
            Based on these analysis, a targeted data augmentation strategy was applied to mitigate dataset
            imbalances, resulting in a final dataset of 8,304 images.
        </p>

    </section>

     <section>
        <h3>Training Details</h3>
        <p>
            The model used for this project is <strong>YOLOv8 nano</strong>, configured for keypoint detection.
            This architecture was chosen instead of <strong>YOLOv11 nano</strong>, because it might have
            been too complex for the limited dataset size, potentially causing overfitting.
        </p>
        
        <p>
            The dataset was divided into a training set of 6,190 images and a validation set of 2,114 images, with no test set,
            because a further division of the data could have reduced the training set size too much, and therefore no
            hyperparameter fine-tuning was performed.
        </p>

        <p>
            Check out the model's performance in the <a href="{{ url_for('results') }}">Results</a> section.
        </p>

    </section>
</section>

<!-- API Documentation -->
<section>
    <h2>API Documentation</h2>

    <section>
        <h3>/predict</h3>
        <p>
            Accepts image uploads via POST and returns the detected keypoints, bounding boxes, and confidence scores.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>Form field: <code>photo</code> (image file)</li>
            <li>Response JSON:
                <pre>
{
    "keypoints": [[x1, y1], [x2, y2], ...],
    "bounding_boxes": [[x, y, w, h], ...],
    "confidence": [...]
}</pre>
            </li>
        </ul>
    </section>

    <section>
        <h3>/clean-keypoints</h3>
        <p>
            Cleans raw predicted keypoints data by following perspective rules, to improve accuracy and consistency.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>JSON body: <pre>{"keypoints": [[x1, y1], ...]}</pre></li>
            <li>Response JSON: <pre>{"keypoints": [[x1, y1], ...]}</pre></li>
        </ul>
    </section>

    <section>
        <h3>/translate-position</h3>
        <p>
            Translates a point from image coordinates (pixel space) to normalized table coordinates.
            The transformation is computed using the last four detected keypoints, corresponding to
            the lower vertices of the table (play area).
        </p>

        <p>
            The input point must be expressed in pixel coordinates relative to the image.
            The output is a normalized 2D point in a coordinate system, where the origin is 
            located at the center of the play area and the <code>x</code> axis goes towards
            the line formed by the keypoint 5 and 6, and the <code>y</code> axis goes towards
            the line formed by the keypoint 4 and 5.
        </p>

        <p>Example:</p>
        <img src="/static/images/page-images/Translated Point.png">

        <ul>
            <li>Method: POST</li>
            <li>JSON body:
                <pre>
{
    "lower_keypoints": [[x1, y1], [x2, y2], ...],
    "point": [x, y]
}</pre>
            </li>
            <li>Response JSON: <pre>{"translated_point": [x, y]}</pre></li>
        </ul>
    </section>

    <section>
        <h3>/get-player-lines</h3>
        <p>
            Calculates the estimated positions of the foosball player lines based on all detected keypoints.
        </p>
        <ul>
            <li>Method: POST</li>
            <li>JSON body: <pre>{"keypoints": [[x1, y1], [x2, y2], ...]}</pre></li>
            <li>Response JSON:
                <pre>
{
    "player_lines": [[[x1, y1], [x2, y2]], ...]
}</pre>
            </li>
        </ul>
    </section>
</section>
{% endblock %}