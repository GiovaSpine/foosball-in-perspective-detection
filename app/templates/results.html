{% extends "base.html" %}

<!-- ========================================================================== -->

{% block head %}
    <link rel="stylesheet" href="{{ url_for('static', filename='info_pages_style.css') }}">
{% endblock %}

<!-- ========================================================================== -->

{% block content %}
<h2>Results</h2>

<section id="metrics">
    <h2>Performance Evaluation</h2>

    <p>
        Although the evaluation reports extremely high performance values, 
        a closer analysis reveals that these metrics alone do not portray
        the generalization capabilities of the model.
    </p>

    <ul>
        <li>precision(B): 0.9976</li>
        <li>recall(B): 0.9999</li>
        <li>mAP50(B): 0.9950</li>
        <li>mAP50-95(B): 0.9279</li>
        <li>precision(P): 0.9976</li>
        <li>recall(P): 0.9999</li>
        <li>mAP50(P): 0.9950</li>
        <li>mAP50-95(P): 0.9934</li>
    </ul>

    <p>
        Despite these near-perfect numerical results, a visual inspection of the
        model predictions highlights clear limitations in its ability to generalize to
        unseen scenarios. We can see some examples below.
    </p>

    <div class="really_big_gallery">
         <img src="/static/images/samples/added17_frame_029_bad.jpg">
        <img src="/static/images/samples/index14_frame_017_bad.jpg">
        <img src="/static/images/samples/index88_frame_020_bad.jpg">
        <img src="/static/images/samples/Foto 24 bad.jpg">
        <img src="/static/images/samples/added27_frame_014_bad.jpg">
    </div>

    <p>
        This discrepancy is likely caused by the limited size and diversity of the training dataset.
        The task involves a wide range of different tables, configurations, camera lenses,
        viewpoints, lighting conditions, and occlusions, which are not sufficiently
        represented in the available data.
        As a result YOLO might be too complex for the dataset, overfitting the training data.
    </p>

    <p>
        Evidence of overfitting can also be observed during training, where the
        pose loss on the training set converges to approximately 0.01, while the
        validation pose loss, around 0.1, indicating a tenfold increase.
        This gap suggests that the model has learned to fit the training samples
        very accurately but struggles to generalize beyond them.
    </p>

</section>


<div class="big_results">
    <h3>Labels</h3>
    <img src="/static/images/samples/val_batch0_labels.jpg">

    <h3>Predictions</h3>
    <img src="/static/images/samples/val_batch0_pred.jpg">

    <h3>Results</h3>
    <img src="/static/images/graphs/results.png">
</div>


<section>
    <h2>Qualitative Analysis and Expected Behavior</h2>
    <p>
        Beyond quantitative metrics, it is also important to evaluate whether the model behaves
        consistently with the expectations derived from the data analysis and augmentation process.
        In this section, qualitative examples are used to assess whether the model performs better
        in well-represented scenarios and degrades in cases that were identified as challenging.
    </p>
    <ul>
        <li>
            As expected, the model performs poorly on images with a not vertical direction (angle different from 90 degrees). 
            This behavior is consistent with the dataset distribution, as there are very few images with table orientations 
            differing from 90 degrees, and the model has therefore seen limited examples of such cases during training.
            <div class="really_big_gallery">
                <img src="/static/images/samples/added52_frame_027.jpg">
                <img src="/static/images/samples/added54_frame_049.jpg">
                <img src="/static/images/samples/DataSetVari 49.jpg">
                <img src="/static/images/samples/index69_frame_005.jpg">
                <img src="/static/images/samples/index80_frame_013.jpg">
            </div>
        </li>

        <li>
            As expected, the model performs poorly when at least one of the first four keypoints is occluded (visibilty 1),
            following the analyzed visibilites graph distribution.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index88_frame_023.jpg">
                <img src="/static/images/samples/index88_frame_024.jpg">
            </div>
        </li>

        <li>
            As expected, the model performs poorly when the keypoint 6 or/and 7 are outside the image boundaries (visibilty 0),
            following the analyzed visibilites graph distribution.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index71_frame_017.jpg">
                <img src="/static/images/samples/index83_frame_010.jpg">
            </div>
        </li>

        <li>
            When an obstacle partially covers the table but none of the keypoints, 
            the model's performance can be slightly degraded. 
            To illustrate this, we show a comparison between an image with an obstacle and the corresponding image without it.
            <div class="really_big_gallery">
                <img src="/static/images/samples/Screenshot (646).jpg">
                <img src="/static/images/samples/Screenshot (625).jpg">
            </div>
            <p>
                However, if the obstacle is minor or not very intrusive, the impact on predictions is minimal, 
                and the model still produces accurate keypoint annotations.
            </p>
            <div class="really_big_gallery">
                <img src="/static/images/samples/added29_frame_026.jpg">
                <img src="/static/images/samples/added29_frame_023.jpg">
            </div>
        </li>

        <li>
            We expected the model to perform better when the table is located near the center of the image,
            since most training images are centered. 
            To test this, we can compare predictions on the original centered images with predictions 
            on artificially translated versions, where the table was shifted far from the center(via mirroring and translation).
            The examples below show that the model produces virtually identical predictions in both cases,
            demonstrating robustness to table displacement within the image.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index102_frame_012.jpg">
                <img src="/static/images/samples/index102_frame_012_not_centered.jpg">
            </div>
            <div class="really_big_gallery">
                <img src="/static/images/samples/added33_frame_003.jpg">
                <img src="/static/images/samples/added33_frame_003_not_centered.jpg">
            </div>
        </li>

        <li>
            To evaluate the effect of image shapes on the training, we compare predictions on an horizontal image with
            a vertically cropped version of the same scene.
            The annotations produced by the model are slightly different between the two versions, but this difference
            is very small compared to the large imbalance we had between horizontal and vertical images in the dataset. 
            This suggests that the model has learned to generalize effectively across different table shapes.
            <div class="really_big_gallery">
                <img src="/static/images/samples/added37_frame_009.jpg">
                <img src="/static/images/samples/added37_frame_009_vertical.jpg">
            </div>
            <div class="really_big_gallery">
                <img src="/static/images/samples/added29_frame_019.jpg">
                <img src="/static/images/samples/added29_frame_019_vertical.jpg">
            </div>
        </li>

        <li>
            We expected the model to perform very similarly on both high and low resolution images.
            To evaluate this behavior, we compare predictions obtained from high-resolution images
            with those produced on scaled-down versions of the same images.
            The examples below show that the model generates virtually identical predictions in both cases,
            indicating good robustness to changes in image resolution.
            <div class="really_big_gallery">
                <img src="/static/images/samples/Foto 196 scaled down.jpg">
                <img src="/static/images/samples/Foto 196.jpg">
            </div>
            <div class="really_big_gallery">
                <img src="/static/images/samples/index109_frame_020_scaled_down.jpg">
                <img src="/static/images/samples/index109_frame_020.jpg">
            </div>
        </li>
    </ul>

    <hr>

    <p>
        In addition to the behaviors that were consistent with the expectations derived from the data analysis,
        several unexpected patterns emerged during qualitative evaluation.
        These cases highlight limitations of the model that were not fully anticipated
        based solely on dataset statistics.
    </p>

    <ul>
        <li>
            The model doesn't appear to work well on images captured at a humel eye level viewpoint,
            with a non-zoomed camera (low focal length) of white-colored foosball tables,
            even tough K-Means suggested that a decent amount of these images are present in the dataset.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index2_frame_013.jpg">
                <img src="/static/images/samples/index59_frame_014.jpg">
                <img src="/static/images/samples/added17_frame_029.jpg">
                <img src="/static/images/samples/added27_frame_022.jpg">
                <img src="/static/images/samples/added28_frame_008.jpg">
                <img src="/static/images/samples/Foto 29.jpg">
                <img src="/static/images/samples/index71_frame_003.jpg">
            </div>
        </li>

        <li>
            Overall, the model tends to perform slightly better on images captured from higher viewpoints
            with higher focal lengths, but performance is still not perfect, for white colored-foosball tables.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index86_frame_034.jpg">
                <img src="/static/images/samples/index83_frame_003.jpg">
                <img src="/static/images/samples/index113_frame_020.jpg">
            </div>
        </li>

        <li>
            The issue observed with low focal length and low viewpoint height is not limited to white-colored tables. 
            The examples below illustrate that similar performance degradation occurs with other table colors. 
            <div class="really_big_gallery">
                <img src="/static/images/samples/index14_frame_017.jpg">
                <img src="/static/images/samples/index9_frame_015.jpg">
            </div>
            This is somewhat surprising, as K-Means suggested a relatively large number of images with low focal length in the dataset. 
            The problem seems to be primarily caused by the human eye level viewpoint rather than the focal length itself. 
            This is supported by the example below, where the focal length is still low but the model performs much better.
            <div class="really_big_gallery">
                <img src="/static/images/samples/index113_frame_001.jpg">
            </div>
        </li>

        <li>
            Interestingly, the model appears to perform better on images of darker-colored foosball tables
            captured with a zoomed camera (high focal length) from a high viewpoint.
            K-Means analysis suggested that these images were relatively scarce in the dataset, 
            so one would initially expect worse performance on them.
            <p>
                However, these images are of notably higher quality: they typically have higher resolution, 
                cleaner compositions, and less background noise. They also tend to be more standardized,
                as many are taken during organized tournaments where the camera framing and lighting are controlled,
                rather than casual recordings from smartphones. 
                This higher image quality likely compensates for the lower quantity, allowing the model to perform better
                than expected.
            </p>
            <div class="really_big_gallery">
                <img src="/static/images/samples/index102_frame_006.jpg">
                <img src="/static/images/samples/index104_frame_004.jpg">
                <img src="/static/images/samples/index6_frame_020.jpg">
                <img src="/static/images/samples/added33_frame_005.jpg">
                <img src="/static/images/samples/added37_frame_005.jpg">
            </div>
        </li>

    </ul>

    <hr>

    <h3>Conclusions and Recommendations for Optimal Predictions</h3>
    
    <p>
        Based on the qualitative analysis, the model performs best under the following conditions:
    </p>

    <ul>
        <li>
            <strong>Table direction:</strong> The model requires the foosball table to be vertical in the image, 
            as most training images have this orientation, resulting in poor performance if this condition is not met. 
        </li>
        <li>
            <strong>Keypoints visibility:</strong> The model requires the first four keypoints to always be visible, and
            the keypoints 6 and 7 to be in the image boundaries, as if these conditions are not met, the model will
            perform poorly.
        </li>
        <li>
            <strong>Table occlusion:</strong> The model is robust to obstacles covering part of the table (without occluding any keypoint).
            However, this robustness has limits: as the size or severity of the occlusion increases, the model's performance
            progressively degrades, leading to less accurate keypoint predictions.
        </li>
        <li>
            <strong>Table position:</strong> The model is robust to different positions in the image.
        </li>
        <li>
            <strong>Image shape:</strong> The model is robust to different image shapes.
        </li>
        <li>
            <strong>Resolution:</strong> The model is robust to different image resolutions.
        </li>
        <li>
            <strong>Viewpoint and focal length:</strong> Images captured from higher viewpoints and with moderate to high zoom (focal length) 
            tend to give the best predictions, while very low viewpoints or low focal length images may lead to degraded performance.
        </li>
        <li>
            <strong>Table color:</strong> While the model has learned to generalize across different table colors, 
            darker tables with good contrast against the background tend to be predicted more reliably.
        </li>
    </ul>

    <p>
        Following these guidelines should help users obtain the best possible predictions from the model.
    </p>

</section>

{% endblock %}